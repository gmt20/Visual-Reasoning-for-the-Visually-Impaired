---
layout: default
---
<body>
    <section class="main-content">
      <h1 id="cs7641-project" align="center">CS7641 - Project Team 17</h1>

<h2 id="introductionbackground">Introduction</h2>
<p align="justify">At present, the visually impaired people use a simple stick for navigation. However, the use of such a stick does not enable them to navigate independently. If they ever get into an unknown environment, the most that they can do, without any external help, is detect stationary obstacles around them. Unfortunately, they cannot make any decision based on a comprehensive understanding of the environment.</p>
<p align="justify">To address this problem, we intend to develop a Machine Learning model for generating accurate visual understanding of a given scene. A model of this kind can potentially be integrated into an e-stick which assists the visually impaired and enables them to move with the same ease and confidence as normally sighted people.</p>

<h2 id="problemstatement">Problem Definition</h2>
<p align="justify">Our project aims at developing a software framework that can detect objects from images and then answer questions based on the content of those images. From a big picture perspective, this project is a stepping stone towards engineering a system that can capture real-time images to provide high-level contextual information about the surroundings. In the future, combining such a system with a mapping and navigation module and integrating into an e-stick would enable the visually impaired to navigate independently.</p>
	
<h3 id="VQA">Dataset</h3>  
      
<p align="justify">To resolve the visual challenges faced by the visually impaired people in their day-to-day lives, we present a Machine Learning model based on the <a href="https://vizwiz.org/tasks-and-datasets/vqa/">Vizwiz</a> dataset. The training dataset consists of photos taken by the blind people annotated with the question asked relevant to that image. Each annotation question  also consists of answers and answer types specified by 10 people for each sample. This provides an opportunity as well a challenge to assist the visually impaired to help them in navigation, assisting their daily life tasks and answering their visual questions etc. The original Vizwiz dataset consisted of :
</p>
<li>20,523 training image/question pairs</li>
<li>4,319 validation image/question pairs</li>
<li>8,000 test image/question pairs</li>
 
<h1 id="Methods">Methods</h2>
<h2 id="Step 1">Step 1. Data Preprocessing</h2>
        
<h3> Feature Extraction using Convolutional Neural Networks </h3>       
        
<p align="justify">We use transfer learning to create feature vectors for the images present in the dataset. The activations from the last layer of different 3 pre-trained models like Inceptionv3, ResNet (Residual Network) and VGG16 (Very Deep Convolutional Networks for Large-Scale Image Recognition), which are state-of-the-art and are widely used. These models are pre-trained on ImageNet, which is a huge image dataset containing more than 14 million images. Hence, they can be used for our task to create feature vectors of the images considered for our task from the Vizwiz dataset.
</p>       
        
<p align="justify"> We first ran the Inceptionv3 using CPU and GPU, with and without batching. The time taken for each of these operations are displayed in Table 1.
	We also tried out different batching sizes while running the feature extraction, this information is displayed in Table 2. 
</p>
<br/>
<table>
  <caption>Table 1: Time Taken with Different Configuration</caption>
  <tr>
    <th>CPU/GPU used </th>
    <th>Time</th>
  </tr>
  <tr>
    <td>CPU feature extraction time</td>
    <td>1 h 42 min</td>
  </tr>
  <tr>
    <td>CPU feature extraction time</td>
    <td>43 min</td>
  </tr>
</table>
 </br>
</br>
<table>
  <caption>Table 2: Execution Time vs Batching Size</caption>
  <tr>
    <th>Batching Sizes</th>
    <th>Time</th>
  </tr>
  <tr>
    <td>4</td>
    <td>8m 2s</td>
  </tr>
  <tr>
    <td>8</td>
    <td>5m 9s</td>
  </tr>
  <tr>
    <td>16</td>
    <td>5m 14s</td>
  </tr>
  <tr>
    <td>32</td>
    <td>5m 11s</td>
  </tr>
  <tr>
    <td>64</td>
    <td>5m 2s</td>
  </tr>
</table>	    
<br/>
<h2 id="Clustering">Unsupervised Algorithm:  Clustering </h2>
    
<p align="justify">On closely examining the dataset, we observed that few images are totally blurred, few are black/white and few others just have too much flash. Hence we realised the need to clean the dataset before feeding it to our training pipeline. 
</p>

<h4>Challenge 1: </h4>
        <p>The first challenge that we faced is how to identify the images to be discarded.</p>
        
<h4>Solution:</h4>
<li align="justify">To identify such groups of images, we decided to perform K-means clustering on the original image dataset to generate similar clusters.</li>
<li align="justify">After generating clusters of similar images, we manually went through each of the clusters, and discarded the clusters that seemed useless for our use case.</li>
<li align="justify">The discarded clusters include clusters having blurred/ unclear images.</li>

<h4>Challenge 2: </h4>    
        <p align="justify">K-means clustering uses Euclidean distance as the metric to determine the similarity between different images. But some images might contain similar objects but they might be present in different orientations/positions/color contrasts, etc. Hence considering just the euclidean distance between the pixel values is not a good metric. 
        </p>

<h4>Solution:</h4>

<p align="justify">We used CNN to detect and classify the objects and accordingly performed the clustering on these CNN generated feature vectors.
Each CNN layer performs optimized operations like centering the objects, gray-scale conversion(normalization), detecting the objects, and accordingly classifying and clustering the images.
</p>
    
<h4>Challenge 3: </h4>   
<p align="justify">The next challenge that we faced was to identify the correct value of K(number of clusters) to be generated.</p>

<h4>Solution:</h4>

<p align="justify">To identify the accurate number of clusters to be considered, we calculated the loss values for a variety of distinct K values.
We then plotted each of these values and used the Elbow method to determine the best value of K to be considered for our use case.</p>

<p align="justify">The following table depicts the loss values obtained for each value of k:</p>

<table>
  <caption>Table 3: Loss Value vs Number of Clusters</caption>
    <tr>
    <th>Value of K (Number of Clusters)</th>
    <th>Loss Value</th>
  </tr>
  <tr>
    <td>1</td>
    <td>7674</td>
  </tr>
  <tr>
    <td>2</td>
    <td>7501</td>
  </tr>
  <tr>
    <td>3</td>
    <td>7411</td>
  </tr>
  <tr>
    <td>5</td>
    <td>7268</td>
  </tr>
  <tr>
    <td>10</td>
    <td>7051</td>
  </tr>
  <tr>
    <td>20</td>
    <td>6830</td>
  </tr>
  <tr>
    <td>40</td>
    <td>6612</td>
  </tr>
  <tr>
    <td>60</td>
    <td>6477</td>
  </tr>
  <tr>
    <td>300</td>
    <td>5957</td>
  </tr>
  <tr>
    <td>1000</td>
    <td>5494</td>
  </tr>
</table>

<p align="justify">The following is the elbow curve we generated based on the above values:</p>

<figure>
  <img src="Picture5.png" alt="KMeans Elbow Curve" style="width:100%">
  <figcaption align="center">Figure 1: KMeans Elbow Curve</figcaption>
</figure>

<p align="justify">Hence, based on the elbow method, we obtained the accurate value of K as 60. Therefore, we generated 60 clusters. We then manually visited each of the clusters(i.e. each of the 60 folders of images), and discarded a few clusters to create a clean image dataset.

In this way, the dataset was reduced from 20,523 image/question pairs to 13000 image/question pairs.
</p>

<p align="justify">The following images show samples that were discarded:</p>
<figure>
  <img src="Picture1.jpg" alt="Discarded Cluster Sample 1" width="500" height="600">
  <figcaption align="center">Figure 2: Discarded Cluster Sample 1</figcaption>
</figure>
<figure>
  <img src="Picture2.jpg" alt="Discarded Cluster Sample 2" width="500" height="600">
  <figcaption align="center">Figure 3: Discarded Cluster Sample 2</figcaption>
</figure>

<p align="justify">The following images show samples that were included:</p>
<figure>
  <img src="Picture3.jpg" alt="Included Cluster Sample 1" width="500" height="600">
  <figcaption align="center">Figure 4: Included Cluster Sample 1</figcaption>
</figure>
<figure>
  <img src="Picture4.jpg" alt="Included Cluster Sample 2" width="500" height="600">
  <figcaption align="center">Figure 5: Included Cluster Sample 2</figcaption>
</figure>


<h4>Results of Data preprocessing</h4>
<p align="justify">To overcome challenges 1,2 and 3 we ran K-means on feature vectors generated by a forward pass of a Convolutional Neural Network to generate 60 clusters, and out of those 60 clusters, we kept 29 clusters and discarded the rest based on the reasoning provided before, thus generating a dataset consisting of 13000 image/question pairs. 
</p>

<h4>Annotation Script</h4>

<p align="justify">After generating 29 folders(clusters), we had to use only the images from these folders. To uniquely read images from 29 out of 60 folders, we wrote an annotation script.
</p>
<p align="justify">The script reads the annotation json file, which consists of the following structure:</p>

<pre class="line-numbers">
   <code class="language-css">
{
Image_name : “ “, // Image name
“Question: “ “, // The question associated with each image
“Answers” : {
		// List of answers from 10 people
	      }
“Answer_type”:[ yes/no], [other], [unanswerable],
“Answerable” : 0/1
}
  </code>
</pre>

<p align="justify">From this file, only those images will be considered that are present in the 29 clusters, and for those image names, we read questions and answers associated with them. All this data is then loaded into a new json file.
</p>
<p align="justify">This new json file is then used for further processing.</p>
</p>

<h3> Verification of Clustering using PCA and T-SNE</h3>
Post k-means clustering, we selected a few clusters manually. In order to verify that the clusters selected manually were accurate, we used PCA to reduce the dimensionality and then t-SNE to visualize the high-dimensional data of images into clusters. 

<h4>Principle Component Analysis(PCA)</h4>
<p align="justify">PCA is the process of dimensionality reduction in which we can project each data point onto only to the first few principal components to obtain lower-dimensional data while preserving maximum datas variation. 
Here, we use PCA to reduce the dimensionality of the feature vectors of the images, so that the feature vectors can then be given as input to T-SNE.
</p>

<h4>T-SNE:</h4>
<p align="justify">T-SNE is an unsupervised, non-linear technique primarily used for data exploration and visualizing high-dimensional data. The t-SNE algorithm calculates a similarity measure between pairs of instances in the high dimensional space and in the low dimensional space. It then tries to optimize these two similarity measures using a cost function. t-SNE differs from PCA by preserving only small pairwise distances or local similarities whereas PCA is concerned with preserving large pairwise distances to maximize variance.
</p>

<p align="justify">In the figure below we can see that the clusters selected manually are well-defined, and hence we can clearly state that the clustering has been done accurately. We also tried tuning the hyperparameters by giving different values for (no. of clusters), to obtain different results.</p>
<figure>
  <img src="tsne.png" alt="t-distributed stochastic neighbor embedding" style="width:100%">
  <figcaption align="center">Figure 14: t-distributed stochastic neighbor embedding (t-SNE)</figcaption>
</figure>

<h3>Average and Max Pooling</h3>

<p align="justify">We also experimented taking max pooling and average pooling post the convolution layers for both Inceptionv3 and ResNet models, the results of which are shared in Tables 4, 5, 6, 7 and 8.
</p>

<h3>Resnet V/s Inception</h3>
<h4>InceptionV3_Max_Normalized</h4>

<table>
  <caption>Table 4: InceptionV3 Max Normalized</caption>
  <tr>
    <th>Value of K (Number of Clusters)</th>
    <th>Loss Value</th>
  </tr>
  <tr>
    <td>1</td>
    <td>7691</td>
  </tr>
  <tr>
    <td>2</td>
    <td>7508</td>
  </tr>
  <tr>
    <td>3</td>
    <td>7392</td>
  </tr>
  <tr>
    <td>5</td>
    <td>7226</td>
  </tr>
  <tr>
    <td>10</td>
    <td>6991</td>
  </tr>
  <tr>
    <td>20</td>
    <td>6747</td>
  </tr>
  <tr>
    <td>40</td>
    <td>6499</td>
  </tr>
  <tr>
    <td>60</td>
    <td>6356</td>
  </tr>
  <tr>
    <td>300</td>
    <td>5796</td>
  </tr>
  <tr>
    <td>1000</td>
    <td>5322</td>
  </tr>
</table>

<figure>
  <img src="Picture6.png" alt="KMeans Elbow Curve (InceptionV3 Max Normalized)" style="width:100%">
  <figcaption align="center">Figure 6: KMeans Elbow Curve (InceptionV3 Max Normalized)</figcaption>
</figure>

<h4>InceptionV3_Avg_Normalized</h4>

<table>
  <caption>Table 5: InceptionV3 Avg Normalized</caption>
  <tr>
    <th>Value of K (Number of Clusters)</th>
    <th>Loss Value</th>
  </tr>
  <tr>
    <td>1</td>
    <td>11171</td>
  </tr>
  <tr>
    <td>2</td>
    <td>10832</td>
  </tr>
  <tr>
    <td>3</td>
    <td>10630</td>
  </tr>
  <tr>
    <td>5</td>
    <td>10307</td>
  </tr>
  <tr>
    <td>10</td>
    <td>9913</td>
  </tr>
  <tr>
    <td>20</td>
    <td>9508</td>
  </tr>
  <tr>
    <td>40</td>
    <td>9046</td>
  </tr>
  <tr>
    <td>60</td>
    <td>8788</td>
  </tr>
  <tr>
    <td>300</td>
    <td>7813</td>
  </tr>
  <tr>
    <td>1000</td>
    <td>7029</td>
  </tr>
</table>

<figure>
  <img src="Picture7.png" alt="KMeans Elbow Curve (InceptionV3 Avg Normalized)" style="width:100%">
  <figcaption align="center">Figure 7: KMeans Elbow Curve (InceptionV3 Avg Normalized)</figcaption>
</figure>

<h4>InceptionV3_Max_Unnormalized</h4>

<table>
  <caption>Table 6: InceptionV3 Max Unnormalized</caption>
  <tr>
    <th>Value of K (Number of Clusters)</th>
    <th>Loss Value</th>
  </tr>
  <tr>
    <td>1</td>
    <td>114840986</td>
  </tr>
  <tr>
    <td>2</td>
    <td>10739880</td>
  </tr>
  <tr>
    <td>3</td>
    <td>105134892</td>
  </tr>
  <tr>
    <td>5</td>
    <td>102756470</td>
  </tr>
  <tr>
    <td>10</td>
    <td>99803204</td>
  </tr>
  <tr>
    <td>20</td>
    <td>96764906</td>
  </tr>
  <tr>
    <td>40</td>
    <td>93636811</td>
  </tr>
  <tr>
    <td>60</td>
    <td>91779277</td>
  </tr>
  <tr>
    <td>300</td>
    <td>84182650</td>
  </tr>
  <tr>
    <td>1000</td>
    <td>76288868</td>
  </tr>
</table>

<figure>
  <img src="Picture8.png" alt="KMeans Elbow Curve (InceptionV3 Max Unnormalized)" style="width:100%">
  <figcaption align="center">Figure 8: KMeans Elbow Curve (InceptionV3 Max Unnormalized)</figcaption>
</figure>

<h4>InceptionV3_Avg_Unnormalized</h4>
<br/>
<table>
  <caption>Table 7: InceptionV3 Avg Unnormalized</caption>
  <tr>
    <th>Value of K (Number of Clusters)</th>
    <th>Loss Value</th>
  </tr>
  <tr>
    <td>1</td>
    <td>5165279</td>
  </tr>
  <tr>
    <td>2</td>
    <td>4968424</td>
  </tr>
  <tr>
    <td>3</td>
    <td>4867648</td>
  </tr>
  <tr>
    <td>5</td>
    <td>4738606</td>
  </tr>
  <tr>
    <td>10</td>
    <td>4554972</td>
  </tr>
  <tr>
    <td>20</td>
    <td>4392889</td>
  </tr>
  <tr>
    <td>40</td>
    <td>4207680</td>
  </tr>
  <tr>
    <td>60</td>
    <td>4094941</td>
  </tr>
  <tr>
    <td>300</td>
    <td>3667674</td>
  </tr>
  <tr>
    <td>1000</td>
    <td>3312151</td>
  </tr>
</table>
</br>
<figure>
  <img src="Picture9.png" alt="KMeans Elbow Curve (InceptionV3 Avg Unnormalized)" style="width:100%">
  <figcaption align="center">Figure 9: KMeans Elbow Curve (InceptionV3 Avg Unnormalized)</figcaption>
</figure>

<h4>ResNet50_Max_Normalized</h4>
</br>
<table>
  <caption>Table 8: ResNet50 Max Normalized</caption>
  <tr>
    <th>Value of K (Number of Clusters)</th>
    <th>Loss Value</th>
  </tr>
  <tr>
    <td>1</td>
    <td>2840</td>
  </tr>
  <tr>
    <td>2</td>
    <td>2218</td>
  </tr>
  <tr>
    <td>3</td>
    <td>2026</td>
  </tr>
  <tr>
    <td>5</td>
    <td>1855</td>
  </tr>
  <tr>
    <td>10</td>
    <td>1679</td>
  </tr>
  <tr>
    <td>20</td>
    <td>1537</td>
  </tr>
  <tr>
    <td>40</td>
    <td>1421</td>
  </tr>
  <tr>
    <td>60</td>
    <td>1361</td>
  </tr>
  <tr>
    <td>300</td>
    <td>1146</td>
  </tr>
  <tr>
    <td>1000</td>
    <td>960</td>
  </tr>
</table>
<br/>
<figure>
  <img src="Picture10.png" alt="KMeans Elbow Curve (ResNet50 Max Normalized)" style="width:100%">
  <figcaption align="center">Figure 7: KMeans Elbow Curve (ResNet50 Max Normalized)</figcaption>
</figure>

<p align="justify">We finally use Inceptionv3 with max-pooling and run it on GPU using 32-sized batches. K-Means is run on these image feature vectors and based on the loss values received, we selected this architecture. 
</p>

<h3>2. Training Pipeline:</h3>
<p align="justify">After choosing the relevant clusters, the next task is to represent the training data to be fed into the neural network. We will be using the CNN features  along with Question Text features as our input features. We will fuse these vectors together by simply stacking the matrices.
</p>

<li align="justify">Image Representation: The CNN feature that we extracted in the above step for performing clustering (to run K-Means on) is able to capture the image features and is a good representation for our images. The size of each feature vector of an image is 2048.</li>
<li align="justify">Text Representation: To model question text, we initially used the bag-of-words (BOW) technique. The BOW is a way of extracting features from text (frequency) to use in modeling. The size of the embedding of each of the questions is 3042. It involves two things, a vocabulary of known words (question vocabulary) and measure of the presence of these words. We further shifted from Bag of words to BERT to generate word embeddings and analyze context in the given questions.</li>

<h2>Question Representation</h2>

<h3>Bag of Words</h3>
<p align="justify">To generate BOW for questions, we have used the following steps:</p>

<li align="justify">We applied standard preprocessing steps such as tokenizing the question texts, lower-casing all the tokens and removing all punctuation marks. We did not remove stop words (like what, how etc.) as these words appear initially in many questions and sometimes are highly correlated to the answer to a question. </li>
<li align="justify">After preprocessing, we get 3042 unique words in question vocabulary. </li>
<li align="justify">Therefore, we will represent each question using a vector of size 3042, where each column represents a unique word. For each question, we put the frequency of each word present in the question text. Thus, this generates a bag-of-words for the questions in the dataset.</li>

<h3>Limitations of Bag of Words</h3>
Using the bag-of-words technique has the following two limitations:
<li align="justify"> Generation of Sparse Represenatations: For very large input data, the resultant vectors generated will be of large dimensions, which would in turn contain large number of null values. Hence this would lead to the generation of sparse vectors, thus blowing up the feature space.</li>
<li align="justify"> Missing context: Bag of words does a very poor job in analyzing the context of the data. Here, using BERT, an attention-based deep learning model would help to solve the problem of contextual awareness.</li>


<h3>BERT Question Embeddings</h3>
<p align="justify">We use SentenceBERT to encode all questions into 768-dimensional word embeddings. SentenceBERT takes sentences as an input (it can take a sentence pair as well). These sentences are then passed into a BERT model and a pooling layer to generate embeddings. 
BERT stands for Bidirectional Encoder Representations from Transformers. It is a transformer-based machine learning technique for natural language processing pre-training developed by Google. SentenceBERT uses siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. 
Using sentence embeddings in BERT requires pre-processing such as adding tokens like [CLS] and [SEP] tokens, denoting the start and end of the sentence. We use the BERT tokenizer to tokenize the sentences. We take the last hidden layer of SentenceBERT to compute the embeddings. These embeddings are of dimension 768 for each question sentence. 
</p>

<h2>Answer Representation</h2>
<p align="justify">Similar to Question text representation, we also need to model answer text. The unique thing about the Vizwiz dataset is that for each question, we have 10 answers that are annotated. So we have multiple ground truth labels for each question. We decided not to use One hot encoding because that would lead to very high dimensional label vectors, blowing up the feature space and making the model suffer from the curse of dimensionality. Thus, we followed the following steps:
</p>

<li align="justify">We combined answers of all the questions together to form a large corpus. </li>
<li align="justify">We used similar preprocessing steps (like used for questions) such as tokenizing the answer lists and removing all punctuation marks.</li> 
<li align="justify">After the preprocessing, we count the frequency of each word in this text corpus and choose the top K most frequent words. Here we have taken top-K(=3000) unique words from the answer vocabulary.</li>
<li align="justify">So, we create a vector of size K for the answer label for each question.</li>
<li align="justify">We represent this K sized answer label for each question by creating the vector with the count of each of  words that appear in top K words in the 10 answers.</li>

<h3>Architecture</h3>

<p align="justify">As discussed in the training pipeline, we have the image + question embedding for each of the (image, question) pairs. For image embeddings, we use the frozen parameters from the Inceptionv3 model learned on ImageNet classification, and no fine-tuning was performed.</p>

<h4> Previous Architecture using Bag-of-Words</h4>
<p align="justify">We concatenate the BOW questions and images (2048 + 3042), to get a resultant vector of dimension 5090. We use this combined input for the Multi-Layer Perceptron (MLP). MLP is a fully connected neural network classifier consisting of 4 hidden layers with 5090 hidden units. We use ReLU for activation, which is finally followed by a log softmax layer to obtain a probability distribution over the top K answers. Since the VizWiz dataset has 10 ground-truth answers for each training instance, we use a "soft" cross-entropy loss so that the model optimises the weights by considering each ground-truth answer.
</p>
<figure>
  <img src="Screenshot 2021-12-07 at 8.04.07 PM.png" alt="Training Pipeline Architecture" style="width:100%">
  <figcaption align="center">Figure 11: Training Pipeline Architecture</figcaption>
</figure>

<h4> Current Architecture using BERT Embeddings</h4>
<p align="justify">in the current architecture we integrate the BERT question embeddings and images (2048 + 768), to get a resultant vector of dimension 2816. 
Here, we have generated results by changing the number of hidden layers, hidden units, neurons, drop-out rates, learning rates as well as with/without regularization.
Similar to the previous architecture, we use ReLU as the activation function, which is followed by a log softmax layer to obtain a probability distribution over the top K answers. 
</p>
<figure>
  <img src="Screenshot 2021-12-07 at 8.05.43 PM.png" alt="Training Pipeline Architecture" style="width:100%">
  <figcaption align="center">Figure 12: Training Pipeline Architecture</figcaption>
</figure>

<h3>Loss Function Used</h3>
<p align="justify">The proposed loss function, termed as soft cross entropy, is a simple weighted average of each unique ground-truth answer. </p>

<figure>
  <img src="loss_function.png" alt="Loss Function Equation">
</figure>

<p align="justify">here c is a vector of unique ground-truth answers and w is a vector of answer weights computed as the number of times the unique answer appears in the ground-truth set divided by the total number of answers.
</p>

<h3>Accuracy Metric Used</h3>
<p align="justify">The 10 answers given by humans for each visual question can differ and therefore, a prediction to a visual question can be 100% correct if the answer has at least 3 occurrences, ∼67% if the occurrences are exactly 2, ∼33% if the answer appears only once in the sample annotations.
Hence, we use the following accuracy metric:</p>
<figure>
  <img src="accuracy_function.png" alt="Loss Function Equation">
</figure>


<h3>Preliminary Results for Mid-term</h3>
<p align="justify">The following are the preliminary results of training on randomly selected samples from the filtered VizWiz dataset.
</p>
<li>Epochs: 150, Training Instances: 10000, Validation Instances: 3108, Optimiser: Adam, Learning Rate: 1e-6</li>
<li>Hidden Layers: 4, Activation: ReLU</li>
<figure>
  <img src="e150_h4_lr1e-6_b128_norm_train.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 13: Train Loss</figcaption>
</figure>
<figure>
  <img src="e150_h4_lr1e-6_b128_norm_val.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 14: Validation Loss</figcaption>
</figure>

<h3>Final Results</h3>

<h4>BERT and BOW Models without dropout and regularization</h4>
<h5>Model 1: Baseline Accuracy Graph for epochs = 100, 4 Hidden Layers for training dataset without dropout and regularization</h5>

<figure>
  <img src="Image1_Baseline.jpeg" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 15: Validation Loss</figcaption>
</figure>

<h5>Model 2: Baseline Accuracy Graph for epochs = 100, 4 Hidden Layers for validation dataset without dropout and regularization</h5>

<figure>
  <img src="Image2_Baseline.jpeg" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 16: Validation Loss</figcaption>
</figure>

<h5>Model 3: BERT Accuracy Graph for epochs = 100, 4 Hidden Layers for training dataset without dropout and regularization</h5>

<figure>
  <img src="Image_1_Bert.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 17: Validation Loss</figcaption>
</figure>

<h5>Model 4: BERT Accuracy Graph for epochs = 100, 4 Hidden Layers for validation dataset without dropout and regularization</h5>

<figure>
  <img src="Image2_Bert.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 18: Validation Loss</figcaption>
</figure>

<h4>BERT and BOW Models with dropout and regularization</h4>

<h5>Model 1: 4 Hidden Layers with BOW with Learning rate set to 1e-6.</h5>

<figure>
  <img src="E300_H4_LR1e-6_relu_B128_norm_BERT.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 19: Validation Loss</figcaption>
</figure>

<h5>Model 2: 4 Hidden Layers with BERT embeddings with Learning rate set to 1e-5.</h5>

<figure>
  <img src="bert_lr1e-5_train.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 20: Validation Loss</figcaption>
</figure>

<figure>
  <img src="bert_lr1e-5_val.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 21: Validation Loss</figcaption>
</figure>

<h5>Model 3: 4 Hidden Layers with BERT embeddings with Learning rate set to 1e-4.</h5>

<figure>
  <img src="bert_lr1-e4.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 22: Validation Loss</figcaption>
</figure>

<figure>
  <img src="bert_lr1-e4.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 23: Validation Loss</figcaption>
</figure>

<h5>Model 4: 1 Hidden Layer with BERT embeddings , Learning rate set to 1e-5 and number of neurons set to 5000.</h5>

<figure>
  <img src="bert_h1_lr1e-5_dropout_train.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 24: Validation Loss</figcaption>
</figure>

<figure>
  <img src="bert_h1_lr1e-5_dropout_val.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 25: Validation Loss</figcaption>
</figure>

<h5>Model 5: 1 Hidden Layer with BERT embeddings, drop out layers(0.5) , Learning rate set to 1e-5 and number of neurons set to 3000 and L2 regularization(1e-5).</h5>

<figure>
  <img src="BERT_LR1e-5_H1-3000_Dropout_L2-1e-5.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 26: Validation Loss</figcaption>
</figure>

<figure>
  <img src="BERT_LR1e-5_H1-3000_Dropout_L2-1e-5_val.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 27: Validation Loss</figcaption>
</figure>

<h5>Model 6: 1 Hidden Layer with BERT embeddings, drop out layers(0.5) , Learning rate set to 1e-5 and number of neurons set to 1024 and L2 regularization(1e-5).</h5>

<figure>
  <img src="BERT_LR1e-5_H1-1024_Dropout_L2-1e-5.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 28: Validation Loss</figcaption>
</figure>

<h5>Model 7: 1 Hidden Layer with BERT embeddings, drop out layers(0.5) , Learning rate set to 1e-5 and number of neurons set to 1024 and L2 regularization(1e-5) only on training loss.</h5>

<figure>
  <img src="BERT_H1-1024_LR1e-5_Dropout_L2-1e-5-Only-Train.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 29: Validation Loss</figcaption>
</figure>

<h5>Model 8: 1 Hidden Layer with BERT embeddings, drop out layers(0.5) , Learning rate set to 1e-5 and number of neurons set to 512 and L2 regularization(1e-5).</h5>

<figure>
  <img src="BERT_LR1e-5_L2-1e-5_both_H1-512.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 30: Validation Loss</figcaption>
</figure>

<h5>Model 9: 1 Hidden Layer with BERT embeddings, drop out layers(0.5) , Learning rate set to 1e-5 and number of neurons set to 256 and L2 regularization(1e-5).</h5>

<figure>
  <img src="BERT_LR1e-5_L2-1e-5_both_H1-256.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 31: Validation Loss</figcaption>
</figure>

<h5>Model 10: 2 Hidden Layers with BERT embeddings, drop out layers(0.5) , Learning rate set to 1e-5 and number of neurons set to 128 and L2 regularization(1e-5).</h5>

<figure>
  <img src="BERT_LR1e-5_L2-1e-5_both_H2-128.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 32: Validation Loss</figcaption>
</figure>

<h5>Model 11: 1 Hidden Layers with BERT embeddings, drop out layers(0.5) , Learning rate set to 1e-5 and number of neurons set to 128 and L2 regularization(1e-5).</h5>

<figure>
  <img src="BERT_LR1e-5_L2-1e-5_both_H1-128.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 33: Validation Loss</figcaption>
</figure>

<h3>Accuracy Analysis:</h3>
<p align="justify">The baseline accuracy is around 33%, and the final accuracy of the model with BERT is 35.8%. We see only a slight increase in the accuracy. Since the visually impaired people take pictures as well as ask questions themselves, the dataset being used is a challenging dataset for modern vision algorithms. It imposes the following challenges for model's training:</p>
<li align="justify">Images are often of poor quality due to low focus and poor lighting, which might lead to less accurate results.</li>
<li align="justify">Questions are on average more conversational/absurd and in some cases are incomplete due to audio recording imperfections.</li>
<li align="justify">Visually impaired people are also not able to verify the correctness of image captured which sometimes may lead to mismatch between image and question asked.</li>
<li align="justify">We are using pre-trained CNN architectures such as Inceptionv3 and ResNet for creating image features and pretrained BERT sentence transformers to create text features. We have not tuned these features to our dataset. And this might lead to a slightly lower accuracy than expected.</li>
<li align="justify">We attribute the poor generalisation of this algorithm largely to its inability to predict answers observed in the dataset because only 824 out of the top 3000 answers in Vizwiz are included in the dataset used to train the model.</li>

<h3>Conclusion and Future Scope</h3>
<p align="justify">In this way our project aims at developing a software framework that can detect objects from images and then answer questions based on the content of those images. From a big picture perspective, this project provides high-level contextual information about the surroundings to the visually impaired.</p>
<p align="justify">Following are some ways we can work on the future aspects of the project:</p>
<li align="justify">To improve the dataset, data augmentation can be performed, such as adding data from other datasets. We can also work on adding further images using translation, rotation, flipping etc. And work on adding questions and answer sets in a similar way. </li>
<li align="justify">Once accuracy is improved, our model can be utilized by the visually impaired for navigation purposes and performing daily chores. We can also request their feedback and input on developing the model </li>
<li align="justify">Currently the dataset includes static images. In the future, video based analysis can be done, to make the system applicable to dynamic real time use cases. </li>
<li align="justify">Our project currently takes input (questions) in a text format and delivers output (answers) also in the text format. In the future, for the ease of the visually impaired people, there needs to be a speech to text system in place which can convert the questions asked by the blind people to text and there should also be a text to speech system which can convert the answers back to speech and inform the blind people.</li>

<h2>References</h3>
<li align="justify">Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., & Parikh, D. (2015). Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision (pp. 2425-2433).</li>
<li align="justify">Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., & Wojna, Z. (2016). Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2818-2826).</li>
<li align="justify">He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).</li>
<li align="justify">Ilievski, I., & Feng, J. (2017). A simple loss function for improving the convergence and accuracy of visual question answering models. arXiv preprint arXiv:1708.00584.</li>
<li align="justify">Dushi, D. (2019). Using Deep Learning to Answer Visual Questions from Blind People.</li>
<li align="justify">Zeeshan Saquib, et. al.BlinDar: An Invisible Eye for the Blind People, IEEE International Conference On Recent Trends In Electronics Information Communication Technology, India, May 2017.</li>
<li align="justify"><a href="https://www.image-net.org">ImageNet</a>
<li align="justify">Reimers, N., & Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-networks</li>


</body>
    
