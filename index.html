---
layout: default
---
<body>
    <section class="main-content">
      <h1 id="cs7641-project" align="center">CS7641 - Project Team 17</h1>

<h2 id="introductionbackground">Introduction</h2>
<p align="justify">At present, the visually impaired people use a simple stick for navigation. However, the use of such a stick does not enable them to navigate independently. If they ever get into an unknown environment, the most that they can do, without any external help, is detect stationary obstacles around them. Unfortunately, they cannot make any decision based on a comprehensive understanding of the environment.</p>
<p align="justify">To address this problem, we intend to develop a Machine Learning model for generating accurate visual understanding of a given scene. A model of this kind can potentially be integrated into an e-stick which assists the visually impaired and enables them to move with the same ease and confidence as normally sighted people.</p>

<h2 id="problemstatement">Problem Definition</h2>
<p align="justify">Our project aims at developing a software framework that can detect objects from images and then answer questions based on the content of those images. From a big picture perspective, this project is a stepping stone towards engineering a system that can capture real-time images to provide high-level contextual information about the surroundings. In the future, combining such a system with a mapping and navigation module and integrating into an e-stick would enable the visually impaired to navigate independently.</p>
	
<h3 id="VQA">Dataset</h3>  
      
<p align="justify">To resolve the visual challenges faced by the visually impaired people in their day-to-day lives, we present a Machine Learning model based on the <a href="https://vizwiz.org/tasks-and-datasets/vqa/">Vizwiz</a> dataset. The training dataset consists of photos taken by the blind people annotated with the question asked relevant to that image. Each annotation question  also consists of answers and answer types specified by 10 people for each sample. This provides an opportunity as well a challenge to assist the visually impaired to help them in navigation, assisting their daily life tasks and answering their visual questions etc. The original Vizwiz dataset consisted of :
</p>
<li>20,523 training image/question pairs</li>
<li>4,319 validation image/question pairs</li>
<li>8,000 test image/question pairs</li>
 
<h1 id="Methods">Methods</h2>
<h2 id="Step 1">Step 1. Data Preprocessing</h2>
        
<h3> Feature Extraction using Convolutional Neural Networks </h3>       
        
<p align="justify">We use transfer learning to create feature vectors for the images present in the dataset. The activations from the last layer of different 3 pre-trained models like Inceptionv3, ResNet (Residual Network) and VGG16 (Very Deep Convolutional Networks for Large-Scale Image Recognition), which are state-of-the-art and are widely used. These models are pre-trained on ImageNet, which is a huge image dataset containing more than 14 million images. Hence, they can be used for our task to create feature vectors of the images considered for our task from the Vizwiz dataset.
</p>       
        
<p align="justify"> We first ran the Inceptionv3 using CPU and GPU, with and without batching. The time taken for each of these operations are displayed in Table 1.
	We also tried out different batching sizes while running the feature extraction, this information is displayed in Table 2. 
</p>
<br/>
<table>
  <caption>Table 1: Time Taken with Different Configuration</caption>
  <tr>
    <th>CPU/GPU used </th>
    <th>Time</th>
  </tr>
  <tr>
    <td>CPU feature extraction time</td>
    <td>1 h 42 min</td>
  </tr>
  <tr>
    <td>CPU feature extraction time</td>
    <td>43 min</td>
  </tr>
</table>
 </br>
</br>
<table>
  <caption>Table 2: Execution Time vs Batching Size</caption>
  <tr>
    <th>Batching Sizes</th>
    <th>Time</th>
  </tr>
  <tr>
    <td>4</td>
    <td>8m 2s</td>
  </tr>
  <tr>
    <td>8</td>
    <td>5m 9s</td>
  </tr>
  <tr>
    <td>16</td>
    <td>5m 14s</td>
  </tr>
  <tr>
    <td>32</td>
    <td>5m 11s</td>
  </tr>
  <tr>
    <td>64</td>
    <td>5m 2s</td>
  </tr>
</table>	    
<br/>
<h2 id="Clustering">Unsupervised Algorithm:  Clustering </h2>
    
<p align="justify">On closely examining the dataset, we observed that few images are totally blurred, few are black/white and few others just have too much flash. Hence we realised the need to clean the dataset before feeding it to our training pipeline. 
</p>

<h4>Challenge 1: </h4>
        <p>The first challenge that we faced is how to identify the images to be discarded.</p>
        
<h4>Solution:</h4>
<li>To identify such groups of images, we decided to perform K-means clustering on the original image dataset to generate similar clusters.</li>
</li>After generating clusters of similar images, we manually went through each of the clusters, and discarded the clusters that seemed useless for our use case.</li>
<li>The discarded clusters include clusters having blurred/ unclear images.</li>

<h4>Challenge 2: </h4>    
        <p>K-means clustering uses Euclidean distance as the metric to determine the similarity between different images. But some images might contain similar objects but they might be present in different orientations/positions/color contrasts, etc. Hence considering just the euclidean distance between the pixel values is not a good metric. 
        </p>

<h4>Solution:</h4>

<p align="justify">We used CNN to detect and classify the objects and accordingly performed the clustering on these CNN generated feature vectors.
Each CNN layer performs optimized operations like centering the objects, gray-scale conversion(normalization), detecting the objects, and accordingly classifying and clustering the images.
</p>
    
<h4>Challenge 3: </h4>   
<p>The next challenge that we faced was to identify the correct value of K(number of clusters) to be generated.</p>

<h4>Solution:</h4>

<p align="justify">To identify the accurate number of clusters to be considered, we calculated the loss values for a variety of distinct K values.
We then plotted each of these values and used the Elbow method to determine the best value of K to be considered for our use case.</p>

<p align="justify">The following table depicts the loss values obtained for each value of k:</p>

<table>
  <caption>Table 3: Loss Value vs Number of Clusters</caption>
    <tr>
    <th>Value of K (Number of Clusters)</th>
    <th>Loss Value</th>
  </tr>
  <tr>
    <td>1</td>
    <td>7674</td>
  </tr>
  <tr>
    <td>2</td>
    <td>7501</td>
  </tr>
  <tr>
    <td>3</td>
    <td>7411</td>
  </tr>
  <tr>
    <td>5</td>
    <td>7268</td>
  </tr>
  <tr>
    <td>10</td>
    <td>7051</td>
  </tr>
  <tr>
    <td>20</td>
    <td>6830</td>
  </tr>
  <tr>
    <td>40</td>
    <td>6612</td>
  </tr>
  <tr>
    <td>60</td>
    <td>6477</td>
  </tr>
  <tr>
    <td>300</td>
    <td>5957</td>
  </tr>
  <tr>
    <td>1000</td>
    <td>5494</td>
  </tr>
</table>

<p align="justify">The following is the elbow curve we generated based on the above values:</p>

<figure>
  <img src="Picture5.png" alt="KMeans Elbow Curve" style="width:100%">
  <figcaption align="center">Figure 1: KMeans Elbow Curve</figcaption>
</figure>

<p align="justify">Hence, based on the elbow method, we obtained the accurate value of K as 60. Therefore, we generated 60 clusters. We then manually visited each of the clusters(i.e. each of the 60 folders of images), and discarded a few clusters to create a clean image dataset.

In this way, the dataset was reduced from 20,523 image/question pairs to 13000 image/question pairs.
</p>

<p align="justify">The following images show samples that were discarded:</p>
<figure>
  <img src="Picture1.jpg" alt="Discarded Cluster Sample 1" width="500" height="600">
  <figcaption align="center">Figure 2: Discarded Cluster Sample 1</figcaption>
</figure>
<figure>
  <img src="Picture2.jpg" alt="Discarded Cluster Sample 2" width="500" height="600">
  <figcaption align="center">Figure 3: Discarded Cluster Sample 2</figcaption>
</figure>

<p align="justify">The following images show samples that were included:</p>
<figure>
  <img src="Picture3.jpg" alt="Included Cluster Sample 1" width="500" height="600">
  <figcaption align="center">Figure 4: Included Cluster Sample 1</figcaption>
</figure>
<figure>
  <img src="Picture4.jpg" alt="Included Cluster Sample 2" width="500" height="600">
  <figcaption align="center">Figure 5: Included Cluster Sample 2</figcaption>
</figure>


<h4>Results of Data preprocessing</h4>
<p align="justify">To overcome challenges 1,2 and 3 we ran K-means on feature vectors generated by a forward pass of a Convolutional Neural Network to generate 60 clusters, and out of those 60 clusters, we kept 29 clusters and discarded the rest based on the reasoning provided before, thus generating a dataset consisting of 13000 image/question pairs. 
</p>

<h4>Annotation Script</h4>

<p align="justify">After generating 29 folders(clusters), we had to use only the images from these folders. To uniquely read images from 29 out of 60 folders, we wrote an annotation script.
</p>
<p align="justify">The script reads the annotation json file, which consists of the following structure:</p>

<pre class="line-numbers">
   <code class="language-css">
{
Image_name : “ “, // Image name
“Question: “ “, // The question associated with each image
“Answers” : {
		// List of answers from 10 people
	      }
“Answer_type”:[ yes/no], [other], [unanswerable],
“Answerable” : 0/1
}
  </code>
</pre>

<p align="justify">From this file, only those images will be considered that are present in the 29 clusters, and for those image names, we read questions and answers associated with them. All this data is then loaded into a new json file.
</p>
<p align="justify">This new json file is then used for further processing.</p>
</p>

<h3>Average and Max Pooling</h3>

<p align="justify">We also experimented taking max pooling and average pooling post the convolution layers for both Inceptionv3 and ResNet models, the results of which are shared in Tables 4, 5, 6, 7 and 8.
</p>

<h3>Resnet V/s Inception V/s VGG16</h3>
<h4>InceptionV3_Max_Normalized</h4>

<table>
  <caption>Table 4: InceptionV3 Max Normalized</caption>
  <tr>
    <th>Value of K (Number of Clusters)</th>
    <th>Loss Value</th>
  </tr>
  <tr>
    <td>1</td>
    <td>7691</td>
  </tr>
  <tr>
    <td>2</td>
    <td>7508</td>
  </tr>
  <tr>
    <td>3</td>
    <td>7392</td>
  </tr>
  <tr>
    <td>5</td>
    <td>7226</td>
  </tr>
  <tr>
    <td>10</td>
    <td>6991</td>
  </tr>
  <tr>
    <td>20</td>
    <td>6747</td>
  </tr>
  <tr>
    <td>40</td>
    <td>6499</td>
  </tr>
  <tr>
    <td>60</td>
    <td>6356</td>
  </tr>
  <tr>
    <td>300</td>
    <td>5796</td>
  </tr>
  <tr>
    <td>1000</td>
    <td>5322</td>
  </tr>
</table>

<figure>
  <img src="Picture6.png" alt="KMeans Elbow Curve (InceptionV3 Max Normalized)" style="width:100%">
  <figcaption align="center">Figure 6: KMeans Elbow Curve (InceptionV3 Max Normalized)</figcaption>
</figure>

<h4>InceptionV3_Avg_Normalized</h4>

<table>
  <caption>Table 5: InceptionV3 Avg Normalized</caption>
  <tr>
    <th>Value of K (Number of Clusters)</th>
    <th>Loss Value</th>
  </tr>
  <tr>
    <td>1</td>
    <td>11171</td>
  </tr>
  <tr>
    <td>2</td>
    <td>10832</td>
  </tr>
  <tr>
    <td>3</td>
    <td>10630</td>
  </tr>
  <tr>
    <td>5</td>
    <td>10307</td>
  </tr>
  <tr>
    <td>10</td>
    <td>9913</td>
  </tr>
  <tr>
    <td>20</td>
    <td>9508</td>
  </tr>
  <tr>
    <td>40</td>
    <td>9046</td>
  </tr>
  <tr>
    <td>60</td>
    <td>8788</td>
  </tr>
  <tr>
    <td>300</td>
    <td>7813</td>
  </tr>
  <tr>
    <td>1000</td>
    <td>7029</td>
  </tr>
</table>

<figure>
  <img src="Picture7.png" alt="KMeans Elbow Curve (InceptionV3 Avg Normalized)" style="width:100%">
  <figcaption align="center">Figure 7: KMeans Elbow Curve (InceptionV3 Avg Normalized)</figcaption>
</figure>

<h4>InceptionV3_Max_Unnormalized</h4>

<table>
  <caption>Table 6: InceptionV3 Max Unnormalized</caption>
  <tr>
    <th>Value of K (Number of Clusters)</th>
    <th>Loss Value</th>
  </tr>
  <tr>
    <td>1</td>
    <td>114840986</td>
  </tr>
  <tr>
    <td>2</td>
    <td>10739880</td>
  </tr>
  <tr>
    <td>3</td>
    <td>105134892</td>
  </tr>
  <tr>
    <td>5</td>
    <td>102756470</td>
  </tr>
  <tr>
    <td>10</td>
    <td>99803204</td>
  </tr>
  <tr>
    <td>20</td>
    <td>96764906</td>
  </tr>
  <tr>
    <td>40</td>
    <td>93636811</td>
  </tr>
  <tr>
    <td>60</td>
    <td>91779277</td>
  </tr>
  <tr>
    <td>300</td>
    <td>84182650</td>
  </tr>
  <tr>
    <td>1000</td>
    <td>76288868</td>
  </tr>
</table>

<figure>
  <img src="Picture8.png" alt="KMeans Elbow Curve (InceptionV3 Max Unnormalized)" style="width:100%">
  <figcaption align="center">Figure 8: KMeans Elbow Curve (InceptionV3 Max Unnormalized)</figcaption>
</figure>

<h4>InceptionV3_Avg_Unnormalized</h4>
<br/>
<table>
  <caption>Table 7: InceptionV3 Avg Unnormalized</caption>
  <tr>
    <th>Value of K (Number of Clusters)</th>
    <th>Loss Value</th>
  </tr>
  <tr>
    <td>1</td>
    <td>5165279</td>
  </tr>
  <tr>
    <td>2</td>
    <td>4968424</td>
  </tr>
  <tr>
    <td>3</td>
    <td>4867648</td>
  </tr>
  <tr>
    <td>5</td>
    <td>4738606</td>
  </tr>
  <tr>
    <td>10</td>
    <td>4554972</td>
  </tr>
  <tr>
    <td>20</td>
    <td>4392889</td>
  </tr>
  <tr>
    <td>40</td>
    <td>4207680</td>
  </tr>
  <tr>
    <td>60</td>
    <td>4094941</td>
  </tr>
  <tr>
    <td>300</td>
    <td>3667674</td>
  </tr>
  <tr>
    <td>1000</td>
    <td>3312151</td>
  </tr>
</table>
</br>
<figure>
  <img src="Picture9.png" alt="KMeans Elbow Curve (InceptionV3 Avg Unnormalized)" style="width:100%">
  <figcaption align="center">Figure 9: KMeans Elbow Curve (InceptionV3 Avg Unnormalized)</figcaption>
</figure>

<h4>ResNet50_Max_Normalized</h4>
</br>
<table>
  <caption>Table 8: ResNet50 Max Normalized</caption>
  <tr>
    <th>Value of K (Number of Clusters)</th>
    <th>Loss Value</th>
  </tr>
  <tr>
    <td>1</td>
    <td>2840</td>
  </tr>
  <tr>
    <td>2</td>
    <td>2218</td>
  </tr>
  <tr>
    <td>3</td>
    <td>2026</td>
  </tr>
  <tr>
    <td>5</td>
    <td>1855</td>
  </tr>
  <tr>
    <td>10</td>
    <td>1679</td>
  </tr>
  <tr>
    <td>20</td>
    <td>1537</td>
  </tr>
  <tr>
    <td>40</td>
    <td>1421</td>
  </tr>
  <tr>
    <td>60</td>
    <td>1361</td>
  </tr>
  <tr>
    <td>300</td>
    <td>1146</td>
  </tr>
  <tr>
    <td>1000</td>
    <td>960</td>
  </tr>
</table>
<br/>
<figure>
  <img src="Picture10.png" alt="KMeans Elbow Curve (ResNet50 Max Normalized)" style="width:100%">
  <figcaption align="center">Figure 7: KMeans Elbow Curve (ResNet50 Max Normalized)</figcaption>
</figure>

<p align="justify">We finally use Inceptionv3 with max-pooling and run it on GPU using 32-sized batches. K-Means is run on these image feature vectors and based on the loss values received, we selected this architecture. 
</p>

<h3>2. Training Pipeline:</h3>
<p align="justify">After choosing the relevant clusters, the next task is to represent the training data to be fed into the neural network. We will be using the CNN features  along with Question Text features as our input features. We will fuse these vectors together by simply stacking the matrices.
</p>

<li>Image Representation:
The CNN feature that we extracted in the above step for performing clustering (to run K-Means on) is able to capture the image features and is a good representation for our images. The size of each feature vector of an image is 2048.</li>
<li>Text Representation:
To model question text, we will be using the bag-of-words (BOW) technique. The BOW is a way of extracting features from text (frequency) to use in modeling. The size of the embedding of each of the questions is 3042. It involves two things, a vocabulary of known words (question vocabulary) and measure of the presence of these words.</li> 

<h3>Question BOW Generation</h3>
<p align="justify">To generate BOW for questions, we have used the following steps:</p>

<li>We applied standard preprocessing steps such as tokenizing the question texts, lower-casing all the tokens and removing all punctuation marks. We did not remove stop words (like what, how etc.) as these words appear initially in many questions and sometimes are highly correlated to the answer to a question. [1] 
</li>
<li>After preprocessing, we get 3042 unique words in question vocabulary.</li>
<li>Therefore, we will represent each question using a vector of size 3042, where each column represents a unique word. For each question, we put the frequency of each word present in the question text. Thus, this generates a bag-of-words for the questions in the dataset.
</li>

<h3>BERT Question Embeddings</h3>
<p align="justify">We use SentenceBERT to encode all questions into 768-dimensional word embeddings. SentenceBERT takes sentences as an input (it can take a sentence pair as well). These sentences are then passed into a BERT model and a pooling layer to generate embeddings. 
BERT stands for Bidirectional Encoder Representations from Transformers. It is a transformer-based machine learning technique for natural language processing pre-training developed by Google. SentenceBERT uses siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. 
Using sentence embeddings in BERT requires pre-processing such as adding tokens like [CLS] and [SEP] tokens, denoting the start and end of the sentence. We use the BERT tokenizer to tokenize the sentences. We take the last hidden layer of SentenceBERT to compute the embeddings. These embeddings are of dimension 768 for each question sentence. 
</p>

<h3>Answer Representation</h3>
<p align="justify">Similar to Question text representation, we also need to model answer text. The unique thing about the Vizwiz dataset is that for each question, we have 10 answers that are annotated. So we have multiple ground truth labels for each question. We decided not to use One hot encoding because that would lead to very high dimensional label vectors, blowing up the feature space making the model suffer from the curse of dimensionality. Thus, we followed the following steps:
</p>

<li>We combined answers of all the questions together to form a large corpus. </li>
<li>We used similar preprocessing steps (like used for questions) such as tokenizing the answer lists and removing all punctuation marks.</li> 
<li>After the preprocessing, we take top-K(=3000) unique words in answer vocabulary.</li>
<li>Then we count the frequency of each word in this text corpus and choose the top K most frequent words.</li>
<li>So, we create a vector of size K for the answer label for each question.</li>
<li>We represent this K sized answer label for each question by creating the vector with the count of each of  words that appear in top K words in the 10 answers.</li>

<h3>Architecture</h3>
<p align="justify">As discussed in the training pipeline, we have the image + question embedding for each of the (image, question) pairs. For image embeddings, we use the frozen parameters from the Inceptionv3 model learned on ImageNet classification, and no fine-tuning was performed. 
We concatenate the BOW questions and images (2048 + 3042), to get a resultant vector of dimension 5090. We use this combined input for the Multi-Layer Perceptron (MLP). MLP is a fully connected neural network classifier consisting of 4 hidden layers with 5090 hidden units. We use ReLU for activation, which is finally followed by a  log softmax layer to obtain a probability distribution over the top K answers. Since the VizWiz dataset has 10 ground-truth answers for each training instance, we use a "soft" cross-entropy loss so that the model optimises the weights by considering each ground-truth answer. The team is currently working on the outlined MLP pipeline which we plan to use as a baseline for future extensions. 
</p>
<figure>
  <img src="Picture11.png" alt="Training Pipeline Architecture" style="width:100%">
  <figcaption align="center">Figure 11: Training Pipeline Architecture</figcaption>
</figure>

<h3>Loss Function</h3>
<p align="justify">
	
The proposed loss function, termed as soft cross entropy, is a simple weighted average of each unique ground-truth answer. 

<img src="https://render.githubusercontent.com/render/math?math=L(x, c, w) = \sum_{i=1}^{|c|}w_i(\− x_{c_i} + log(\sum_{j=1}^{|x|}exp(x_j)))">

where c is a vector of unique ground-truth answers and w is a vector of answer weights computed as the number of times the unique answer appears in the ground-truth set divided by the total number of answers.

</p>-->

<h3>Preliminary Results</h3>
<p align="justify">The following are the preliminary results of training on randomly selected samples from the filtered VizWiz dataset.
</p>
<li>Epochs: 150, Training Instances: 10000, Validation Instances: 3108, Optimiser: Adam, Learning Rate: 1e-6</li>
<li>Hidden Layers: 4, Activation: ReLU</li>
<figure>
  <img src="e150_h4_lr1e-6_b128_norm_train.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 12: Train Loss</figcaption>
</figure>
<figure>
  <img src="e150_h4_lr1e-6_b128_norm_val.png" alt="Preliminary Results" style="width:100%">
  <figcaption align="center">Figure 13: Validation Loss</figcaption>
</figure>

<h3>Accuracy Metric Used</h3>
<p align="justify">The 10 answers given by humans for each visual question can differ and therefore, a prediction to a visual question can be 100% correct if the answer has at least 3 occurrences, ∼67% if the occurrences are exactly 2, ∼33% if the answer appears only once in the sample annotations.
</p>
<p align="justify">Hence, we use the following accuracy metric:</p>
<p align="justify">Accuracy = min(number of humans that provided that answer / 3 , 1)</p>

<h4>t-SNE:</h4>
<p align="justify">t-SNE is an unsupervised, non-linear technique primarily used for data exploration and visualizing high-dimensional data. The t-SNE algorithm calculates a similarity measure between pairs of instances in the high dimensional space and in the low dimensional space. It then tries to optimize these two similarity measures using a cost function. t-SNE differs from PCA by preserving only small pairwise distances or local similarities whereas PCA is concerned with preserving large pairwise distances to maximize variance.</p>
<p align="justify">In order to verify that the clusters selected manually were accurate, we used PCA to reduce the dimensionality and then t-SNE to visualize the high-dimensional data.</p>
<p align="justify">In the figure below we can see that the clusters are well-defined, and hence we can clearly state that the clustering has been done accurately. We also tried tuning the hyperparameters by giving different values for (no. of clusters), to obtain different results.</p>
<figure>
  <img src="tsne.png" alt="t-distributed stochastic neighbor embedding" style="width:100%">
  <figcaption align="center">Figure 14: t-distributed stochastic neighbor embedding (t-SNE)</figcaption>
</figure>









<h3>Future Scope</h3>
<p align="justify">For the future scope of the project, we intend to do the following to improve accuracy:
</p>
<li>We plan to use other CNN architectures such as ResNet and check if we can arrive at better results.</li>
<li>We plan to use the LSTM for representing our question text data. Unlike the bag of words technique we use currently, LSTM will be able to take meaning in consideration. If we use appropriate layers of embedding and encoding in LSTM, the model will be able to find out the actual meaning in input string and will give the most accurate output class.</li>

<h2>References</h3>
<li>Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., & Parikh, D. (2015). Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision (pp. 2425-2433).</li>
<li>Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., & Wojna, Z. (2016). Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2818-2826).</li>
<li>He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).</li>
<li>Ilievski, I., & Feng, J. (2017). A simple loss function for improving the convergence and accuracy of visual question answering models. arXiv preprint arXiv:1708.00584.</li>
<li>Dushi, D. (2019). Using Deep Learning to Answer Visual Questions from Blind People.</li>
<li>Zeeshan Saquib, et. al.BlinDar: An Invisible Eye for the Blind People, IEEE International Conference On Recent Trends In Electronics Information Communication Technology, India, May 2017.</li>
<li><a href="https://www.image-net.org">ImageNet</a>

</body>
    
